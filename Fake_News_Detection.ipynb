{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f081165",
      "metadata": {},
      "source": [
        "# **Fake News Detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4a40b9-6445-4faa-9806-cda649000dc7",
      "metadata": {
        "id": "8e4a40b9-6445-4faa-9806-cda649000dc7"
      },
      "source": [
        "## Step 1: Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b17edfb7-4042-4e48-92c2-8d337de48571",
      "metadata": {
        "id": "b17edfb7-4042-4e48-92c2-8d337de48571",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28882a0e-ca28-4209-a08e-6c3a94995bd5",
      "metadata": {
        "id": "28882a0e-ca28-4209-a08e-6c3a94995bd5"
      },
      "source": [
        "## Step 2: Download NLTK resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617f0aff-a564-4c48-af82-d07e5c77d40b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "617f0aff-a564-4c48-af82-d07e5c77d40b",
        "outputId": "8bb9cb09-18a1-44cb-88e3-9c66016da943"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de30b2b-d905-4ab9-8b29-d6206794f2c9",
      "metadata": {
        "id": "0de30b2b-d905-4ab9-8b29-d6206794f2c9"
      },
      "source": [
        "## Step 3: Download and extract the LIAR dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1da0c35-4984-45a2-acb5-5901dc0fa37d",
      "metadata": {
        "id": "f1da0c35-4984-45a2-acb5-5901dc0fa37d"
      },
      "outputs": [],
      "source": [
        "def download_liar_dataset():\n",
        "    url = \"https://www.cs.ucsb.edu/~william/data/liar_dataset.zip\"\n",
        "    dataset_path = r\"C:\\Users\\varav\\Documents\\Project\\Fake News Detection\\liar_dataset.zip\"\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(\"Downloading LIAR dataset...\")\n",
        "        urllib.request.urlretrieve(url, dataset_path)\n",
        "        with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"liar_dataset\")\n",
        "        print(\"Dataset extracted.\")\n",
        "    else:\n",
        "        print(\"LIAR dataset already downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d17f0392-a396-430d-86d3-4f3760870ae1",
      "metadata": {
        "id": "d17f0392-a396-430d-86d3-4f3760870ae1"
      },
      "source": [
        "## Step 4: Load and preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d191c79a-7a43-461c-8f6d-326e353488ee",
      "metadata": {
        "id": "d191c79a-7a43-461c-8f6d-326e353488ee"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    train_path = \"liar_dataset/train.tsv\"\n",
        "    valid_path = \"liar_dataset/valid.tsv\"\n",
        "    test_path = \"liar_dataset/test.tsv\"\n",
        "\n",
        "    train_df = pd.read_csv(train_path, sep='\\t', header=None)\n",
        "    valid_df = pd.read_csv(valid_path, sep='\\t', header=None)\n",
        "    test_df = pd.read_csv(test_path, sep='\\t', header=None)\n",
        "\n",
        "    columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job', 'state', 'party',\n",
        "               'barely_true', 'false', 'half_true', 'mostly_true', 'pants_fire', 'context']\n",
        "    train_df.columns = columns\n",
        "    valid_df.columns = columns\n",
        "    test_df.columns = columns\n",
        "\n",
        "    df = pd.concat([train_df, valid_df], ignore_index=True)\n",
        "\n",
        "    def simplify_label(label):\n",
        "        if label in ['true', 'mostly-true', 'half-true']:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    df['binary_label'] = df['label'].apply(simplify_label)\n",
        "    test_df['binary_label'] = test_df['label'].apply(simplify_label)\n",
        "\n",
        "    return df[['statement', 'binary_label']], test_df[['statement', 'binary_label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68097869-813d-4d9b-81f4-91275522f9e3",
      "metadata": {
        "id": "68097869-813d-4d9b-81f4-91275522f9e3"
      },
      "source": [
        "## Step 5: Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b639b09-4caa-4ddd-b4e0-9f2d27237c5c",
      "metadata": {
        "id": "7b639b09-4caa-4ddd-b4e0-9f2d27237c5c"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36070d7e-a6f3-40bd-a78a-d78fb28c2349",
      "metadata": {
        "id": "36070d7e-a6f3-40bd-a78a-d78fb28c2349"
      },
      "source": [
        "## Step 6: Feature extraction and model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b7f48e1-e208-445b-b05a-97a3c3861256",
      "metadata": {
        "id": "9b7f48e1-e208-445b-b05a-97a3c3861256"
      },
      "outputs": [],
      "source": [
        "def train_model(train_df, test_df):\n",
        "    train_df['processed_statement'] = train_df['statement'].apply(preprocess_text)\n",
        "    test_df['processed_statement'] = test_df['statement'].apply(preprocess_text)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "    X_train = vectorizer.fit_transform(train_df['processed_statement'])\n",
        "    X_test = vectorizer.transform(test_df['processed_statement'])\n",
        "\n",
        "    y_train = train_df['binary_label']\n",
        "    y_test = test_df['binary_label']\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model, vectorizer, X_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b968922f-ce5e-4ac9-a07d-e7c0d17f865e",
      "metadata": {
        "id": "b968922f-ce5e-4ac9-a07d-e7c0d17f865e"
      },
      "source": [
        "## Step 7: Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2fc614f-ba92-423e-9d09-6df1f864448b",
      "metadata": {
        "id": "b2fc614f-ba92-423e-9d09-6df1f864448b"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69faca9c-8a25-46ce-8a22-899e7efd0f6a",
      "metadata": {
        "id": "69faca9c-8a25-46ce-8a22-899e7efd0f6a"
      },
      "source": [
        "## Step 8: Inference function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad2b1b0c-85bb-42db-934a-3f1c9956f9b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad2b1b0c-85bb-42db-934a-3f1c9956f9b0",
        "outputId": "cc54297f-d5ea-4af8-c0ba-a7d1b177b4c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LIAR dataset already downloaded.\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.6219\n",
            "Precision: 0.6400\n",
            "Recall: 0.7521\n",
            "F1-Score: 0.6916\n",
            "\n",
            "Confusion Matrix:\n",
            "[[251 302]\n",
            " [177 537]]\n",
            "\n",
            "Sample Prediction: The statement 'The economy is booming with record-low unemployment rates.' is Real.\n"
          ]
        }
      ],
      "source": [
        "def predict_fake_news(model, vectorizer, text):\n",
        "    processed_text = preprocess_text(text)\n",
        "    X = vectorizer.transform([processed_text])\n",
        "    prediction = model.predict(X)[0]\n",
        "    return \"Real\" if prediction == 1 else \"Fake\"\n",
        "\n",
        "def main():\n",
        "    download_liar_dataset()\n",
        "\n",
        "    train_df, test_df = load_data()\n",
        "\n",
        "    model, vectorizer, X_test, y_test = train_model(train_df, test_df)\n",
        "\n",
        "    evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    sample_text = \"The economy is booming with record-low unemployment rates.\"\n",
        "    prediction = predict_fake_news(model, vectorizer, sample_text)\n",
        "    print(f\"\\nSample Prediction: The statement '{sample_text}' is {prediction}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
